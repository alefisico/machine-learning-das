{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense neural network with PyTorch\n",
    "Authors: Javier Duarte, Tyler Mitchell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/algomez/miniconda3/envs/hepenv/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Requirement already satisfied: wheel in /home/algomez/miniconda3/envs/hepenv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /home/algomez/miniconda3/envs/hepenv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.5.1)\n",
      "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/home/algomez/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/home/algomez/.local/lib/python3.10/site-packages/torch/lib/../../nvidia/cublas/lib/libcublas.so.11: undefined symbol: cublasLtGetStatusString, version libcublasLt.so.11",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mroot_pandas\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/__init__.py:191\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# Easy way.  You want this most of the time, because it will prevent\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# C++ symbols from libtorch clobbering C++ symbols from other\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# See Note [Global dependencies]\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[0;32m--> 191\u001b[0m         \u001b[43m_load_global_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/__init__.py:153\u001b[0m, in \u001b[0;36m_load_global_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m here \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18m__file__\u001b[39m)\n\u001b[1;32m    151\u001b[0m lib_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(here), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlib\u001b[39m\u001b[38;5;124m'\u001b[39m, lib_name)\n\u001b[0;32m--> 153\u001b[0m \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRTLD_GLOBAL\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hepenv/lib/python3.10/ctypes/__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: /home/algomez/.local/lib/python3.10/site-packages/torch/lib/../../nvidia/cublas/lib/libcublas.so.11: undefined symbol: cublasLtGetStatusString, version libcublasLt.so.11"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import root_pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy TTree HZZ4LeptonsAnalysisReduced into a pandas DataFrame.\n",
    "filename = {\n",
    "    'VV': root_pandas.read_root('data/ntuple_4mu_VV.root', key='HZZ4LeptonsAnalysisReduced'),\n",
    "    'bkg': root_pandas.read_root('data/ntuple_4mu_bkg.root', key='HZZ4LeptonsAnalysisReduced')\n",
    "}\n",
    "\n",
    "# Drop all variables except for those we want to use when training.\n",
    "VARS = ['f_mass4l','f_massjj']\n",
    "df = {\n",
    "    'VV': filename['VV'][VARS],\n",
    "    'bkg': filename['bkg'][VARS],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the inputs are well behaved.\n",
    "df['VV']= df['VV'][(df['VV'][VARS[0]] > -999) & (df['VV'][VARS[1]] > -999)]\n",
    "df['bkg']= df['bkg'][(df['bkg'][VARS[0]] > -999) & (df['bkg'][VARS[1]] > -999)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add isSignal variable\n",
    "df['VV']['isSignal'] = np.ones(len(df['VV'])) \n",
    "df['bkg']['isSignal'] = np.zeros(len(df['bkg'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine signal and background into one DataFrame then split into input variables and labels.\n",
    "NDIM = len(VARS)\n",
    "df_all = pd.concat([df['VV'],df['bkg']])\n",
    "dataset = df_all.values\n",
    "X = dataset[:,0:NDIM]\n",
    "Y = dataset[:,NDIM]\n",
    "\n",
    "# Split into training and testing data.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=0.2, random_state=7)\n",
    "print(X_train_val)\n",
    "print(X)\n",
    "\n",
    "# preprocessing: standard scalar (reshape inputs to mean=0, variance=1)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train_val)\n",
    "X_train_val = scaler.transform(X_train_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Split again, this time into training and validation data.\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build our model. \n",
    "import torch\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 20),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(20, 20),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(20, 1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Use Binary Cross Entropy as our loss function.\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "# Optimize the model parameters using the Adam optimizer.\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def getBatches(N, tot):\n",
    "    \"\"\"Function to find nearest acceptable batch size\"\"\"\n",
    "    print N, tot\n",
    "    if tot % N == 0:\n",
    "        return N\n",
    "    closest = 1\n",
    "    for i in xrange(1, N):\n",
    "        if tot % i == 0 and abs(N - i) < abs(N-closest):\n",
    "            closest = i\n",
    "    return closest\n",
    "    \n",
    "# Create batches from total data. We have to make sure the batch size is an appropriate divisor of the total \n",
    "# number of training events\n",
    "N = getBatches(1024, len(X_train))\n",
    "unbatched_x = torch.from_numpy(X_train).float()\n",
    "x = unbatched_x.view(-1, N, len(VARS))\n",
    "unbatched_y = torch.from_numpy(Y_train).float()\n",
    "y_b = unbatched_y.view(-1, N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation data ready\n",
    "val_data = torch.from_numpy(X_val).float()\n",
    "val_label = torch.from_numpy(Y_val).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, val_losses = [], []\n",
    "min_loss, stale_epochs = 100., 0\n",
    "\n",
    "# 500 epochs. \n",
    "for t in xrange(500):\n",
    "    batch_loss, val_batch_loss = [], []\n",
    "    for b in xrange(len(x)):\n",
    "        \n",
    "        # Forward pass: make a prediction for each x event in batch b.\n",
    "        y_pred = model(x[b])\n",
    "\n",
    "        # Get the labels.\n",
    "        label = y_b[b]\n",
    "        y = label.view_as(y_pred)  # reshape label data to the shape of y_pred\n",
    "\n",
    "        # Compute and print loss.\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "        # Let's look at the validation set.\n",
    "        \n",
    "        # Torch keeps track of each operation performed on a Tensor, so that it can take the gradient later.\n",
    "        # We don't need to store this information when looking at validation data, so turn it off with\n",
    "        # torch.no_grad().\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Forward pass on validation set.\n",
    "            output = model(val_data)\n",
    "\n",
    "            # Get labels and compute loss again\n",
    "            val_y = val_label.view_as(output)\n",
    "            val_loss = loss_fn(output, val_y)\n",
    "            val_batch_loss.append(val_loss.item())\n",
    "\n",
    "            # Monitor the loss function to prevent overtraining.\n",
    "            if stale_epochs > 20:\n",
    "                break\n",
    "\n",
    "            if val_loss.item() - min_loss < 0:\n",
    "                min_loss = val_loss.item()\n",
    "                stale_epochs = 0\n",
    "            else:\n",
    "                stale_epochs += 1\n",
    "\n",
    "        #print(t, b, loss.item(), val_loss.item())\n",
    "\n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the variables it will update (which are the learnable\n",
    "        # weights of the model). This is because by default, gradients are\n",
    "        # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "        # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model\n",
    "        # parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    losses.append(min(batch_loss))\n",
    "    val_losses.append(min(val_batch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "with torch.no_grad():\n",
    "    # plot loss vs epoch\n",
    "    plt.figure(figsize=(16,8))\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    ax.plot(losses, label='loss')\n",
    "    ax.plot(val_losses, label='val_loss')\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Plot ROC\n",
    "    X_test_in = torch.from_numpy(X_test).float()\n",
    "    Y_predict = model(X_test_in)\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr, tpr, thresholds = roc_curve(Y_test, Y_predict)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(16,8))\n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    ax2.plot(fpr, tpr, lw=2, color='cyan', label='auc = %.3f' % (roc_auc))\n",
    "    ax2.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k', label='random chance')\n",
    "    ax2.set_xlim([0, 1.0])\n",
    "    ax2.set_ylim([0, 1.0])\n",
    "    ax2.set_xlabel('false positive rate')\n",
    "    ax2.set_ylabel('true positive rate')\n",
    "    ax2.set_title('receiver operating curve')\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: We want to make a three 2D numpy arrays: \n",
    "# x values at each (x, y) grid point\n",
    "# y values at each (x, y) grid point\n",
    "# z values (model prediction) at each (x, y) grid point\n",
    "\n",
    "myXI, myYI = np.meshgrid(np.linspace(-2, 2, 200), np.linspace(-2, 2, 200))\n",
    "# print shape\n",
    "print(myXI.shape)\n",
    "\n",
    "myZI = np.zeros(myXI.shape)\n",
    "\n",
    "for i in range(0, len(myXI)):\n",
    "    for j in range(0, len(myYI)):\n",
    "        myXI[i,j] # x value of xi, yj point\n",
    "        myYI[i,j] # y value of xi, yj point\n",
    "        data_point = torch.tensor([myXI[i,j], myYI[i,j]])\n",
    "        myZI[i,j] = model(data_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(myZI)\n",
    "myZI = model(torch.from_numpy(np.c_[myXI.ravel(), myYI.ravel()]).float())\n",
    "myZI = myZI.reshape(myXI.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "plt.figure(figsize=(20,7))\n",
    "\n",
    "# plot contour map of NN output\n",
    "# overlaid with test data points\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "cont_plot = ax.contourf(myXI, myYI, myZI>0.5, cmap=cm, alpha=.8)\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, cmap=cm_bright, edgecolors='k')\n",
    "ax.set_xlim(-2,2)\n",
    "ax.set_ylim(-2,2)\n",
    "ax.set_xlabel(VARS[0])\n",
    "ax.set_ylabel(VARS[1])\n",
    "plt.colorbar(cont_plot,ax=ax, boundaries=[0,1],label='NN output')\n",
    "\n",
    "# plot decision boundary\n",
    "# overlaid with test data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
